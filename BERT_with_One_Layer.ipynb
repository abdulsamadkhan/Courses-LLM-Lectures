{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM74clY/lfhWP5RGH2xzMgj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abdulsamadkhan/Courses-LLM-Lectures/blob/main/BERT_with_One_Layer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Phase 1: Self-Attention Mechanism\n",
        "Implement a self-attention mechanism as the foundational component of BERT."
      ],
      "metadata": {
        "id": "d35duUoPntMW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V88CoVnjnhtc",
        "outputId": "742a2626-3df1-47d3-85bb-eb4a6762cf31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Self-Attention output shape: torch.Size([10, 16, 64])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define the Self-Attention mechanism\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim):\n",
        "        \"\"\"\n",
        "        Initialize the self-attention module.\n",
        "        Args:\n",
        "        - embed_dim: Dimensionality of input embeddings and attention vectors.\n",
        "        \"\"\"\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        # Linear layers to transform input embeddings into Q, K, and V matrices\n",
        "        self.query = nn.Linear(embed_dim, embed_dim)\n",
        "        self.key = nn.Linear(embed_dim, embed_dim)\n",
        "        self.value = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "        # Scale factor to normalize the dot product attention scores\n",
        "        self.scale = torch.sqrt(torch.tensor([embed_dim], dtype=torch.float32))\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Perform self-attention on input x.\n",
        "        Args:\n",
        "        - x: Input tensor of shape (batch_size, sequence_length, embed_dim)\n",
        "        Returns:\n",
        "        - Output tensor after applying self-attention.\n",
        "        \"\"\"\n",
        "        # Project the input x into Q, K, and V matrices\n",
        "        Q = self.query(x)\n",
        "        K = self.key(x)\n",
        "        V = self.value(x)\n",
        "\n",
        "        # Compute attention scores by taking dot product of Q and transposed K, and scaling\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n",
        "\n",
        "        # Apply softmax to get the attention weights\n",
        "        attention_weights = torch.softmax(scores, dim=-1)\n",
        "\n",
        "        # Compute the output as the weighted sum of V using the attention weights\n",
        "        return torch.matmul(attention_weights, V)\n",
        "\n",
        "# Testing Self-Attention\n",
        "embed_dim = 64  # Define embedding dimension\n",
        "x = torch.rand(10, 16, embed_dim)  # Random input tensor of shape (batch_size, sequence_length, embed_dim)\n",
        "\n",
        "# Initialize the self-attention layer\n",
        "self_attention = SelfAttention(embed_dim)\n",
        "\n",
        "# Pass the input through the self-attention layer\n",
        "output = self_attention(x)\n",
        "\n",
        "# Print the output shape to verify the dimensions\n",
        "print(\"Self-Attention output shape:\", output.shape)  # Expected: (batch_size, sequence_length, embed_dim)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 2: Multi-Head Attention\n",
        "Create multi-head attention by concatenating outputs from multiple self-attention heads."
      ],
      "metadata": {
        "id": "FzLMcIGXqcg3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        \"\"\"\n",
        "        Initialize the multi-head attention module.\n",
        "        Args:\n",
        "        - embed_dim: Total dimensionality of input embeddings.\n",
        "        - num_heads: Number of attention heads.\n",
        "        \"\"\"\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "\n",
        "        # Number of attention heads and dimensions per head\n",
        "        self.num_heads = num_heads\n",
        "        self.embed_dim = embed_dim\n",
        "        self.head_dim = embed_dim // num_heads  # Dimension for each head\n",
        "\n",
        "        # Create a list of SelfAttention layers, one for each head\n",
        "        self.attention_heads = nn.ModuleList([SelfAttention(self.head_dim) for _ in range(num_heads)])\n",
        "\n",
        "        # Linear layer to transform concatenated attention outputs back to embed_dim\n",
        "        self.output_linear = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Perform multi-head attention on input x.\n",
        "        Args:\n",
        "        - x: Input tensor of shape (batch_size, sequence_length, embed_dim)\n",
        "        Returns:\n",
        "        - Output tensor after applying multi-head attention.\n",
        "        \"\"\"\n",
        "        # Extract dimensions\n",
        "        batch_size, seq_length, embed_dim = x.size()\n",
        "\n",
        "        # Reshape x to split it for each head: (batch_size, num_heads, seq_length, head_dim)\n",
        "        x = x.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # Apply self-attention to each head individually\n",
        "        # Each attention head processes its part of the input tensor x\n",
        "        attention_outputs = [head(x[:, i]) for i, head in enumerate(self.attention_heads)]\n",
        "\n",
        "        # Concatenate outputs from all heads along the last dimension (head_dim)\n",
        "        concat_output = torch.cat(attention_outputs, dim=-1)\n",
        "\n",
        "        # Apply a linear layer to the concatenated output to return to the original embed_dim size\n",
        "        output = self.output_linear(concat_output)\n",
        "        return output\n",
        "\n",
        "# Testing Multi-Head Attention\n",
        "embed_dim = 64      # Define embedding dimension\n",
        "num_heads = 4       # Define number of heads\n",
        "x = torch.rand(10, 16, embed_dim)  # Random input tensor of shape (batch_size, sequence_length, embed_dim)\n",
        "\n",
        "# Initialize the multi-head attention layer\n",
        "multihead_attention = MultiHeadAttention(embed_dim, num_heads)\n",
        "\n",
        "# Pass the input through the multi-head attention layer\n",
        "output = multihead_attention(x)\n",
        "\n",
        "# Print the output shape to verify the dimensions\n",
        "print(\"Multi-Head Attention output shape:\", output.shape)  # Expected: (batch_size, sequence_length, embed_dim)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "brF9SfiwoK3j",
        "outputId": "94553c55-87b5-4a9d-813b-73a3745695c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Multi-Head Attention output shape: torch.Size([10, 16, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 3: Add Layer Normalization\n"
      ],
      "metadata": {
        "id": "38tsqNzzq0R9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AddNorm(nn.Module):\n",
        "    def __init__(self, embed_dim):\n",
        "        \"\"\"\n",
        "        Initialize the Add & Norm layer.\n",
        "        Args:\n",
        "        - embed_dim: Dimensionality of the input embeddings.\n",
        "        \"\"\"\n",
        "        super(AddNorm, self).__init__()\n",
        "\n",
        "        # Layer normalization to stabilize and scale the combined inputs\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    def forward(self, x, sublayer_output):\n",
        "        \"\"\"\n",
        "        Apply Add & Norm operation.\n",
        "        Args:\n",
        "        - x: Original input tensor of shape (batch_size, sequence_length, embed_dim).\n",
        "        - sublayer_output: Output from a sublayer (e.g., multi-head attention) with the same shape.\n",
        "        Returns:\n",
        "        - Normalized output after adding the original input and the sublayer output.\n",
        "        \"\"\"\n",
        "        # Add the input tensor `x` and the output of the sublayer, then apply layer normalization\n",
        "        return self.norm(x + sublayer_output)\n",
        "\n",
        "# Testing Add & Norm\n",
        "add_norm = AddNorm(embed_dim)  # Initialize Add & Norm layer with embedding dimension\n",
        "\n",
        "# Get output from a multi-head attention layer as sublayer_output\n",
        "sublayer_output = multihead_attention(x)\n",
        "\n",
        "# Apply Add & Norm with original input `x` and the sublayer output\n",
        "output = add_norm(x, sublayer_output)\n",
        "\n",
        "# Print the output shape to verify dimensions\n",
        "print(\"Add & Norm output shape:\", output.shape)  # Expected: (batch_size, sequence_length, embed_dim)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPfw9HnqqQIM",
        "outputId": "ec03f0ba-1c91-405e-8e4e-99486e024569"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Add & Norm output shape: torch.Size([10, 16, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 4: Parallel Feed-Forward Neural Network\n",
        "Add a feed-forward neural network in parallel to the attention mechanism."
      ],
      "metadata": {
        "id": "i8uSu5ipsK09"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, embed_dim, hidden_dim):\n",
        "        \"\"\"\n",
        "        Initialize the Feed-Forward layer.\n",
        "        Args:\n",
        "        - embed_dim: Dimensionality of the input embeddings.\n",
        "        - hidden_dim: Dimensionality of the hidden layer.\n",
        "        \"\"\"\n",
        "        super(FeedForward, self).__init__()\n",
        "\n",
        "        # First linear layer to project the input from embed_dim to hidden_dim\n",
        "        self.fc1 = nn.Linear(embed_dim, hidden_dim)\n",
        "\n",
        "        # Second linear layer to project back from hidden_dim to embed_dim\n",
        "        self.fc2 = nn.Linear(hidden_dim, embed_dim)\n",
        "\n",
        "        # ReLU activation function for introducing non-linearity\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of the feed-forward network.\n",
        "        Args:\n",
        "        - x: Input tensor of shape (batch_size, sequence_length, embed_dim).\n",
        "        Returns:\n",
        "        - Output tensor after applying the two linear layers and ReLU activation.\n",
        "        \"\"\"\n",
        "        # Apply the first linear layer followed by ReLU, then the second linear layer\n",
        "        return self.fc2(self.relu(self.fc1(x)))\n",
        "\n",
        "# Testing Feed-Forward NN\n",
        "feed_forward = FeedForward(embed_dim, hidden_dim=128)  # Initialize Feed-Forward layer with hidden dimension\n",
        "\n",
        "# Pass the output from the previous layer through the Feed-Forward network\n",
        "output = feed_forward(output)\n",
        "\n",
        "# Print the output shape to verify dimensions\n",
        "print(\"Feed-Forward output shape:\", output.shape)  # Expected: (batch_size, sequence_length, embed_dim)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lp7eXGSGrDm8",
        "outputId": "aef87fe0-eda1-49d5-f96b-ec9e43ffdd59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feed-Forward output shape: torch.Size([10, 16, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Phase 5: Complete Transformer Block\n",
        "Combine all components into a single Transformer block."
      ],
      "metadata": {
        "id": "0yWJdzlUr2u8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, hidden_dim):\n",
        "        \"\"\"\n",
        "        Initialize the Transformer block.\n",
        "        Args:\n",
        "        - embed_dim: Dimensionality of the input embeddings.\n",
        "        - num_heads: Number of attention heads for multi-head attention.\n",
        "        - hidden_dim: Dimensionality of the hidden layer in the feed-forward network.\n",
        "        \"\"\"\n",
        "        super(TransformerBlock, self).__init__()\n",
        "\n",
        "        # Multi-head attention layer\n",
        "        self.multihead_attention = MultiHeadAttention(embed_dim, num_heads)\n",
        "\n",
        "        # Add & Norm layer after multi-head attention\n",
        "        self.add_norm1 = AddNorm(embed_dim)\n",
        "\n",
        "        # Feed-forward neural network layer\n",
        "        self.feed_forward = FeedForward(embed_dim, hidden_dim)\n",
        "\n",
        "        # Second Add & Norm layer after the feed-forward network\n",
        "        self.add_norm2 = AddNorm(embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of the Transformer block.\n",
        "        Args:\n",
        "        - x: Input tensor of shape (batch_size, sequence_length, embed_dim).\n",
        "        Returns:\n",
        "        - Output tensor after applying multi-head attention, Add & Norm, feed-forward network, and Add & Norm.\n",
        "        \"\"\"\n",
        "        # Apply multi-head attention to the input\n",
        "        attention_output = self.multihead_attention(x)\n",
        "\n",
        "        # Apply the first Add & Norm layer with the attention output\n",
        "        x = self.add_norm1(x, attention_output)\n",
        "\n",
        "        # Apply the feed-forward network\n",
        "        feed_forward_output = self.feed_forward(x)\n",
        "\n",
        "        # Apply the second Add & Norm layer with the feed-forward output\n",
        "        x = self.add_norm2(x, feed_forward_output)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Testing Transformer Block\n",
        "transformer_block = TransformerBlock(embed_dim, num_heads, hidden_dim=128)  # Initialize Transformer block\n",
        "\n",
        "# Pass input tensor `x` through the Transformer block\n",
        "output = transformer_block(x)\n",
        "\n",
        "# Print the output shape to verify dimensions\n",
        "print(\"Transformer Block output shape:\", output.shape)  # Expected: (batch_size, sequence_length, embed_dim)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNdcoe2_rrYV",
        "outputId": "b3230fe9-a724-4c16-867a-353839d40643"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformer Block output shape: torch.Size([10, 16, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 6: Add Positional Encoding and Embedding Layer\n",
        "Add positional encoding and embedding layer for input tokens."
      ],
      "metadata": {
        "id": "FcwzD6mHuEfV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, embed_dim, max_length=5000):\n",
        "        \"\"\"\n",
        "        Initialize the Positional Encoding.\n",
        "        Args:\n",
        "        - embed_dim: Dimensionality of the embeddings.\n",
        "        - max_length: Maximum length of the sequence to support with positional encodings.\n",
        "        \"\"\"\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        # Create a matrix to hold positional encodings with dimensions (max_length, embed_dim)\n",
        "        self.encoding = torch.zeros(max_length, embed_dim)\n",
        "\n",
        "        # Create a tensor of shape (max_length, 1) with positions ranging from 0 to max_length-1\n",
        "        position = torch.arange(0, max_length).unsqueeze(1)\n",
        "\n",
        "        # Calculate the scaling term for even indices based on the position in the embedding\n",
        "        div_term = torch.exp(torch.arange(0, embed_dim, 2) * -(torch.log(torch.tensor(10000.0)) / embed_dim))\n",
        "\n",
        "        # Apply sine to even indices (0, 2, 4, ...) of the positional encoding\n",
        "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
        "\n",
        "        # Apply cosine to odd indices (1, 3, 5, ...) of the positional encoding\n",
        "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        # Add an extra dimension at the beginning for batch compatibility\n",
        "        self.encoding = self.encoding.unsqueeze(0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of the positional encoding.\n",
        "        Args:\n",
        "        - x: Input tensor of shape (batch_size, sequence_length, embed_dim).\n",
        "        Returns:\n",
        "        - Tensor with positional encoding added to input embeddings.\n",
        "        \"\"\"\n",
        "        # Add positional encoding to the input tensor x\n",
        "        # Only take as many positions as needed based on sequence length of x\n",
        "        return x + self.encoding[:, :x.size(1), :]\n",
        "\n",
        "# Testing Positional Encoding\n",
        "positional_encoding = PositionalEncoding(embed_dim)  # Initialize positional encoding\n",
        "\n",
        "\n",
        "x = torch.rand(10, 16, embed_dim)  # Random input tensor of shape (batch_size, sequence_length, embed_dim)\n",
        "\n",
        "# Apply positional encoding to input tensor x\n",
        "embedded_x = positional_encoding(x)\n",
        "\n",
        "# Print the output shape to verify dimensions\n",
        "print(\"Positional Encoding output shape:\", embedded_x.shape)  # Expected: (batch_size, sequence_length, embed_dim)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Af_0Zz9lsab2",
        "outputId": "edf794be-3c7f-4e1b-a56c-58fe84c9c105"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positional Encoding output shape: torch.Size([10, 16, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 7: Understanding the embedding layer\n"
      ],
      "metadata": {
        "id": "djJ74WXE6eUI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define parameters for the embedding layer\n",
        "vocab_size = 100     # Size of vocabulary (e.g., 100 unique tokens)\n",
        "embed_dim = 8        # Embedding dimension (e.g., each token will be represented by an 8-dimensional vector)\n",
        "\n",
        "# Initialize the embedding layer\n",
        "embedding_layer = nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "# Create a batch of token IDs\n",
        "batch_size = 5       # Number of examples in a batch\n",
        "sequence_length = 4   # Number of tokens in each example\n",
        "input_ids = torch.randint(0, vocab_size, (batch_size, sequence_length))  # Random token IDs in range [0, vocab_size)\n",
        "\n",
        "print(\"Input token IDs:\\n\", input_ids)\n",
        "\n",
        "# Pass the token IDs through the embedding layer\n",
        "output = embedding_layer(input_ids)\n",
        "\n",
        "# Display the shape and content of the output\n",
        "print(\"\\nOutput from the embedding layer:\")\n",
        "print(\"Output shape:\", output.shape)   # Expected shape: (batch_size, sequence_length, embed_dim)\n",
        "print(\"Output values:\\n\", output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRFjANq26rFP",
        "outputId": "8e9fb1d8-679f-4608-e83e-2f4dabeacf84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input token IDs:\n",
            " tensor([[63, 53, 88, 67],\n",
            "        [49, 73, 32, 47],\n",
            "        [45, 64, 92, 24],\n",
            "        [97, 11, 82, 11],\n",
            "        [79, 60, 30, 15]])\n",
            "\n",
            "Output from the embedding layer:\n",
            "Output shape: torch.Size([5, 4, 8])\n",
            "Output values:\n",
            " tensor([[[-1.1734,  1.1417,  1.6486,  0.4211,  0.4692, -0.0909, -0.3167,\n",
            "           1.2432],\n",
            "         [-0.1273, -0.3382, -2.0314, -0.8377,  1.0141, -1.6332,  0.6968,\n",
            "          -0.8438],\n",
            "         [-1.1493, -0.7665, -0.6098, -0.0044, -0.4661,  0.7721,  0.3780,\n",
            "          -0.6827],\n",
            "         [ 2.3378,  0.5129, -0.1060,  0.4381,  1.1368,  0.0957,  0.2459,\n",
            "          -0.9346]],\n",
            "\n",
            "        [[-1.3704,  0.9207, -1.0321,  1.3557,  0.7781,  1.0541,  0.9063,\n",
            "          -0.8551],\n",
            "         [ 0.2555,  0.1818,  0.8608,  0.9628,  1.5397,  0.3957,  0.1870,\n",
            "           0.3106],\n",
            "         [ 1.6036,  1.1730,  1.5289,  0.7095,  0.6508,  0.1734, -0.3149,\n",
            "           0.7287],\n",
            "         [ 1.4319, -0.5518, -0.1282, -0.1049, -0.9707, -1.3494,  0.7407,\n",
            "           0.6971]],\n",
            "\n",
            "        [[ 0.3495,  0.5245,  2.1544, -2.1790, -0.8092, -0.0429, -0.1868,\n",
            "           1.4533],\n",
            "         [ 1.3728, -2.1645, -1.2245, -0.1692,  0.3742,  2.7473,  2.0194,\n",
            "           0.8670],\n",
            "         [ 1.4000, -1.3824, -0.1693,  1.6647,  0.0584,  1.5803,  0.2929,\n",
            "           1.0174],\n",
            "         [ 0.0173,  0.0065, -1.1959,  0.7137,  0.5907,  0.2429, -0.8304,\n",
            "           1.2002]],\n",
            "\n",
            "        [[-0.1742,  0.3610, -0.1909, -0.5550,  0.7208, -0.0998,  0.1938,\n",
            "           0.6511],\n",
            "         [ 0.6540, -0.5903, -0.1322,  1.0091, -0.3558,  0.6227, -0.3276,\n",
            "           0.1347],\n",
            "         [-1.0155, -0.1521, -0.2266, -3.2553, -0.9548, -1.1070, -0.1293,\n",
            "           1.2281],\n",
            "         [ 0.6540, -0.5903, -0.1322,  1.0091, -0.3558,  0.6227, -0.3276,\n",
            "           0.1347]],\n",
            "\n",
            "        [[ 0.0226,  0.9657, -0.1896, -0.3783,  0.6500, -1.7392, -0.3090,\n",
            "           0.4628],\n",
            "         [ 0.0863, -0.9272, -1.0920, -0.3880,  0.2324, -0.5720, -0.2227,\n",
            "           2.2110],\n",
            "         [ 0.1631,  1.0681, -0.0635, -1.9296, -1.0924,  0.5270, -0.0857,\n",
            "          -1.1751],\n",
            "         [ 1.3859, -0.9964,  0.5871, -0.9469, -0.1081, -0.1050, -0.5372,\n",
            "           0.0629]]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 8: Training for Next-Word Prediction\n",
        "Use the Transformer block for next-word prediction.\n",
        "\n",
        "* Tokenize the input text and generate training data.\n",
        "* Pass the input through embedding, positional encoding, and the Transformer block.\n",
        "* Add a linear layer for predictions and train on next-word prediction."
      ],
      "metadata": {
        "id": "aoLzWObduU0Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NextWordPredictionModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_heads, hidden_dim, num_layers):\n",
        "        super(NextWordPredictionModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.positional_encoding = PositionalEncoding(embed_dim)\n",
        "        self.transformer_blocks = nn.ModuleList(\n",
        "            [TransformerBlock(embed_dim, num_heads, hidden_dim) for _ in range(num_layers)]\n",
        "        )\n",
        "        self.linear = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        print(\"shape of embeddings of the input tokesn for the batch\",x.shape)\n",
        "        x = self.positional_encoding(x)\n",
        "        print(\"shape of embeddings of After the positional encoding\",x.shape)\n",
        "\n",
        "\n",
        "        for block in self.transformer_blocks:\n",
        "            x = block(x)\n",
        "        return self.linear(x)\n",
        "\n",
        "# Example training loop\n",
        "model = NextWordPredictionModel(vocab_size=30522, embed_dim=embed_dim, num_heads=num_heads, hidden_dim=128, num_layers=1)\n",
        "\n",
        "#torch.randint: This function generates a tensor filled with random integers from a specified range.\n",
        "# 0 to 30522\n",
        "# Together, (10, 16) indicates we are working with a batch of 10 sequences, where each sequence has 16 tokens (or words).\n",
        "input_ids = torch.randint(0, 30522, (10, 16))  #\n",
        "#print intput_ids in meaninful way\n",
        "print(\"inputs:\", input_ids.shape)  # Expected: (batch_size, sequence_length)\n",
        "output_logits = model(input_ids)\n",
        "print(\"Output logits shape:\", output_logits.shape)  # Expected: (batch_size, sequence_length, vocab_size)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MZz3KFEt29v",
        "outputId": "d9fc70e5-9a07-4ddb-ce84-52b8075d86f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs: torch.Size([10, 16])\n",
            "shape of embeddings of the input tokesn for the batch torch.Size([10, 16, 64])\n",
            "Output logits shape: torch.Size([10, 16, 30522])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KmAgywVpumSP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}